{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "from math import log2\n",
    "from typing import List, Dict, Set, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (40%) Correctly implement the ID3 decision tree algorithm, including the ability to handle unknown attributes (You do not need to handle real valued attributes).  \n",
    "### Code Requirements/Notes:\n",
    "- Use standard information gain as your basic attribute evaluation metric.  (Note that normal ID3 would usually augment information gain with gain ratio or some other mechanism to penalize statistically insignificant attribute splits.) \n",
    "- You are welcome to create other classes and/or functions in addition to the ones provided below. (e.g. If you build out a tree structure, you might create a node class).\n",
    "- It is a good idea to use a simple data set (like the lenses data or the pizza homework), which you can check by hand, to test your algorithm to make sure that it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, available_features: Set[int], col_names: Optional[List[str]]=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.available_features = available_features\n",
    "        self.col_names = col_names\n",
    "        \n",
    "        self.split_feature: int = None\n",
    "        self.children: Dict[Optional[int,str],Node] = None\n",
    "        self.info_gain: float = np.nan\n",
    "        self.prediction = None\n",
    "        \n",
    "        if len(np.unique(y)) == 1:\n",
    "            self.prediction = y[0]\n",
    "        elif available_features != 0:\n",
    "            self.prediciton = y[np.argmax(np.unique(y, return_counts=True)[1])]\n",
    "        if self.col_names is None:\n",
    "            self.col_names = list(range(len(X.T) + 1))\n",
    "            \n",
    "    def __str__(self, indent=0):\n",
    "        if self.prediction is not None or not self.children:\n",
    "            return f\"{'   '*indent}prediction: {self.prediction}\\n\"\n",
    "        output_str = \"\"\n",
    "        for value, child in self.children.items():\n",
    "            output_str += f\"{'   '*indent}{self.col_names[self.split_feature]} = {value}, Information Gain = {round(self.info_gain,3)}\\n\"\n",
    "            output_str += child.__str__(indent+1)\n",
    "        return output_str\n",
    "            \n",
    "    def split(self):\n",
    "        info_gains = self.calc_info_gain()\n",
    "        split_col = min(info_gains, key=info_gains.get)\n",
    "        self.info_gain = self.calc_tot_entropy() - info_gains[split_col]\n",
    "        self._create_children(split_col)\n",
    "        \n",
    "    def calc_tot_entropy(self) -> float:\n",
    "        _, counts = np.unique(self.y, return_counts=True)\n",
    "        return -sum(count/len(self.y)*log2(count/len(self.y)) if count/len(self.y) else 0 for count in counts)\n",
    "    \n",
    "    def calc_info_gain(self) -> Dict[int, float]:\n",
    "        info_gains = dict()\n",
    "        for col in self.available_features:\n",
    "            score = 0\n",
    "            for X_attr in np.unique(self.X[:,col]):\n",
    "                X_cols = np.where(self.X[:,col] == X_attr)[0]\n",
    "                entropies = list()\n",
    "                for Y_attr in np.unique(self.y):\n",
    "                    Y_cols = np.where(self.y == Y_attr)[0]\n",
    "                    val = len([idx for idx in Y_cols if idx in X_cols])/len(X_cols)\n",
    "                    entropies.append(-val * log2(val) if val else 0)\n",
    "                score += len(X_cols)/len(self.X) * sum(entropies)\n",
    "            info_gains[col] = score\n",
    "        return info_gains\n",
    "    \n",
    "    def _create_children(self, col) -> None:\n",
    "        self.children = dict()\n",
    "        self.split_feature = col\n",
    "        new_available = self.available_features - set([col])\n",
    "        for attr in np.unique(self.X[:, col]):\n",
    "            target_rows = np.where(self.X[:, col] == attr)[0]\n",
    "            self.children[attr] = Node(self.X[target_rows,:], self.y[target_rows], new_available, self.col_names)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTClassifier(BaseEstimator,ClassifierMixin):\n",
    "\n",
    "    def __init__(self, data, col_names = None):\n",
    "        self.X = data[:,:-1]\n",
    "        self.y = data[:,-1]\n",
    "        self.root: Node= Node(self.X, self.y, set(range(len(self.X.T))), col_names)\n",
    "        \n",
    "\n",
    "    def fit(self):\n",
    "        curr_node = self.root\n",
    "        if curr_node.prediction is None:\n",
    "            curr_node.split()\n",
    "            for child_node in curr_node.children.values():\n",
    "                self._fit_recursive(child_node)\n",
    "        return self\n",
    "    \n",
    "    def _fit_recursive(self, curr_node):\n",
    "        if curr_node.prediction is None:\n",
    "            curr_node.split()\n",
    "            for child_node in curr_node.children.values():\n",
    "                self._fit_recursive(child_node)\n",
    "        # return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted = list()\n",
    "        for test in X:\n",
    "            predicted.append(self._predict_helper(test, self.root))\n",
    "        return np.array(predicted)\n",
    "    \n",
    "    def _predict_helper(self, test, curr_node):\n",
    "        if curr_node.prediction is not None:\n",
    "            return curr_node.prediction\n",
    "        \n",
    "        child_value = test[curr_node.split_feature]\n",
    "        if child_value not in curr_node.children:\n",
    "            y_counts = dict(zip(*np.unique(curr_node.y, return_counts=True)))\n",
    "            return max(y_counts, key=y_counts.get)\n",
    "        \n",
    "        return self._predict_helper(test, curr_node.children[child_value])\n",
    "    \n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\" Return accuracy(Classification Acc) of model on a given dataset. Must implement own score function.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 1D numpy array of the targets \n",
    "        \"\"\"\n",
    "        y_guess = self.predict(data[:,:-1])\n",
    "        y= data[:,-1]\n",
    "        return sum(y==y_guess)/len(y)\n",
    "        # return [(y_i, y_guess_i) for y_i, y_guess_i in zip(y, y_guess) if y_i != y_guess_i]\n",
    "        # return y==y_guess\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Debug\n",
    "\n",
    "Debug your model by training on the lenses dataset: [Debug Dataset (lenses.arff)](https://byu.instructure.com/courses/14142/files?preview=4622251)\n",
    "\n",
    "Test your model on the lenses test set: [Debug Test Dataset (lenses_test.arff)](https://byu.instructure.com/courses/14142/files?preview=4622254)\n",
    "\n",
    "Parameters:\n",
    "(optional) counts = [3,2,2,2] (You should compute this when you read in the data, before fitting)\n",
    "\n",
    "---\n",
    "\n",
    "Expected Results: Accuracy = [0.33]\n",
    "\n",
    "Predictions should match this file: [Lenses Predictions (pred_lenses.csv)](https://byu.instructure.com/courses/14142/files?preview=4622260)\n",
    "\n",
    "*NOTE: The [Lenses Prediction (pred_lenses.csv)](https://byu.instructure.com/courses/14142/files?preview=4622260) uses the following encoding: soft=2, hard=0, none=1. If your encoding is different, then your output will be different, but not necessarily incorrect.*\n",
    "\n",
    "Split Information Gains (These do not need to be in this exact order):\n",
    "\n",
    "[0.5487949406953987, 0.7704260414863775, 0.3166890883150208, 1.0, 0.4591479170272447, 0.9182958340544894]\n",
    "\n",
    "<!-- You should be able to get about 68% (61%-82%) predictive accuracy on the lenses data -->\n",
    "\n",
    "Here's what your decision tree splits should look like, and the corresponding child node predictions:\n",
    "\n",
    "Decision Tree:\n",
    "<pre>\n",
    "tear_prod_rate = normal:\n",
    "    astigmatism = no:\n",
    "        age = pre_presbyopic:\n",
    "            prediction: soft\n",
    "        age = presbyopic:\n",
    "            spectacle_prescrip = hypermetrope:\n",
    "                prediction: soft\n",
    "            spectacle_prescrip = myope:\n",
    "                prediction: none\n",
    "        age = young:\n",
    "            prediction: soft\n",
    "    astigmatism = yes:\n",
    "        spectacle_prescrip = hypermetrope:\n",
    "            age = pre_presbyopic:\n",
    "                prediction: none\n",
    "            age = presbyopic:\n",
    "                prediction: none\n",
    "            age = young:\n",
    "                prediction: hard\n",
    "        spectacle_prescrip = myope:\n",
    "            prediction: hard\n",
    "tear_prod_rate = reduced:\n",
    "    prediction: none\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score =  0.3333333333333333\n",
      "tear_prod_rate = b'normal', Information Gain = 0.549\n",
      "   astigmatism = b'no', Information Gain = 0.77\n",
      "      age = b'pre_presbyopic', Information Gain = 0.317\n",
      "         prediction: b'soft'\n",
      "      age = b'presbyopic', Information Gain = 0.317\n",
      "         spectacle_prescrip = b'hypermetrope', Information Gain = 1.0\n",
      "            prediction: b'soft'\n",
      "         spectacle_prescrip = b'myope', Information Gain = 1.0\n",
      "            prediction: b'none'\n",
      "      age = b'young', Information Gain = 0.317\n",
      "         prediction: b'soft'\n",
      "   astigmatism = b'yes', Information Gain = 0.77\n",
      "      spectacle_prescrip = b'hypermetrope', Information Gain = 0.459\n",
      "         age = b'pre_presbyopic', Information Gain = 0.918\n",
      "            prediction: b'none'\n",
      "         age = b'presbyopic', Information Gain = 0.918\n",
      "            prediction: b'none'\n",
      "         age = b'young', Information Gain = 0.918\n",
      "            prediction: b'hard'\n",
      "      spectacle_prescrip = b'myope', Information Gain = 0.459\n",
      "         prediction: b'hard'\n",
      "tear_prod_rate = b'reduced', Information Gain = 0.549\n",
      "   prediction: b'none'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load debug training data \n",
    "data, meta = arff.loadarff(\"data/lenses.arff\")\n",
    "data = np.array([[*row] for row in data])\n",
    "data\n",
    "\n",
    "# Train Decision Tree\n",
    "clfr = DTClassifier(data, meta.names())\n",
    "clfr.fit()\n",
    "\n",
    "# Load debug test data\n",
    "test_data, _ = arff.loadarff(\"data/lenses_test.arff\")\n",
    "test_data = np.array([[*row] for row in test_data])\n",
    "print(\"score = \", clfr.score(test_data))\n",
    "\n",
    "print(clfr.root)\n",
    "\n",
    "\n",
    "\n",
    "# Predict and compute model accuracy\n",
    "\n",
    "\n",
    "# Print the information gain of every split you make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional/Additional Debugging Dataset - Pizza Homework\n",
    "# pizza_dataset = np.array([['meat','thin','no veg'],['no meat','deep','no veg'],['no meat','stuffed','veg'],['meat','stuffed','veg'],['meat','deep','no veg'],['meat','deep','veg'],['no meat','thin','veg'],['meat','deep','no veg'],['no meat','thin','no veg']])\n",
    "# pizza_labels = np.array(['great','bad','good','great','good','great','good','good','bad'])\n",
    "# pizza_data = np.append(pizza_dataset.T, pizza_labels).T\n",
    "    \n",
    "# clfr = DTClassifier(pizza_data, [\"meat\", \"crust\", \"Veggies\",\"Quality\"])\n",
    "# clfr.fit()\n",
    "# print(clfr.root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Evaluation\n",
    "\n",
    "We will evaluate your model based on its performance on the zoo dataset. \n",
    "\n",
    "Train your model using this dataset: [Evaluation Train Dataset (zoo.arff)](https://byu.instructure.com/courses/14142/files?preview=4622270)\n",
    "\n",
    "Test your model on this dataset: [Evaluation Test Dataset (zoo_test.arff)](https://byu.instructure.com/courses/14142/files?preview=4622274)\n",
    "\n",
    "Parameters:\n",
    "(optional) counts = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2] (You should compute this when you read in the data, before fitting)\n",
    "\n",
    "---\n",
    "Print out your accuracy on the evaluation test dataset.\n",
    "\n",
    "Print out the information gain of every split you make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legs = b'0', Information Gain = 1.363\n",
      "   fins = b'F', Information Gain = 0.887\n",
      "      toothed = b'F', Information Gain = 0.985\n",
      "         prediction: b'c7'\n",
      "      toothed = b'T', Information Gain = 0.985\n",
      "         prediction: b'c3'\n",
      "   fins = b'T', Information Gain = 0.887\n",
      "      eggs = b'F', Information Gain = 0.696\n",
      "         prediction: b'cT'\n",
      "      eggs = b'T', Information Gain = 0.696\n",
      "         prediction: b'c4'\n",
      "legs = b'2', Information Gain = 1.363\n",
      "   hair = b'F', Information Gain = 0.826\n",
      "      prediction: b'c2'\n",
      "   hair = b'T', Information Gain = 0.826\n",
      "      prediction: b'cT'\n",
      "legs = b'4', Information Gain = 1.363\n",
      "   hair = b'F', Information Gain = 0.689\n",
      "      predator = b'F', Information Gain = 0.863\n",
      "         prediction: b'c3'\n",
      "      predator = b'T', Information Gain = 0.863\n",
      "         toothed = b'F', Information Gain = 0.722\n",
      "            prediction: b'c7'\n",
      "         toothed = b'T', Information Gain = 0.722\n",
      "            prediction: b'c5'\n",
      "   hair = b'T', Information Gain = 0.689\n",
      "      prediction: b'cT'\n",
      "legs = b'5', Information Gain = 1.363\n",
      "   prediction: b'c7'\n",
      "legs = b'6', Information Gain = 1.363\n",
      "   predator = b'F', Information Gain = 0.722\n",
      "      prediction: b'c6'\n",
      "   predator = b'T', Information Gain = 0.722\n",
      "      prediction: b'c7'\n",
      "legs = b'8', Information Gain = 1.363\n",
      "   prediction: b'c7'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation training data\n",
    "train_data, meta = arff.loadarff(\"data/zoo.arff\")\n",
    "train_data = np.array([[*row] for row in train_data])\n",
    "\n",
    "# Train Decision Tree\n",
    "clfr = DTClassifier(train_data, meta.names())\n",
    "clfr.fit()\n",
    "\n",
    "# Load evaluation test data\n",
    "test_data, meta = arff.loadarff(\"data/zoo_test.arff\")\n",
    "test_data = np.array([[*row] for row in test_data])\n",
    "\n",
    "clfr.score(test_data)\n",
    "\n",
    "# Print out the information gain for every split you make\n",
    "print(clfr.root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (20%) You will use your ID3 algorithm to induce decision trees for the cars dataset and the voting dataset.  Do not use a stopping criterion, but induce the tree as far as it can go (until classes are pure or there are no more data or attributes to split on).  \n",
    "- Implement and use 10-fold Cross Validation (CV) on each data set to predict how well the models will do on novel data.  \n",
    "- For each dataset, report the training and test classification accuracy for each fold and the average test accuracy. \n",
    "- As a rough sanity check, typical decision tree accuracies for these data sets are: Cars: .90-.95, Vote: .92-.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement 10-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2 Cars Dataset\n",
    "- Use this [Cars Dataset (cars.arff)](https://byu.instructure.com/courses/14142/files?preview=4622293)\n",
    "- Make a table for your k-fold cross validation accuracies\n",
    "\n",
    "*If you are having trouble using scipy's loadarff function (scipy.io.arff.loadarff), try:*\n",
    "\n",
    "*pip install arff &nbsp;&nbsp;&nbsp;&nbsp;          # Install arff library*\n",
    "\n",
    "*import arff as arf*                   \n",
    "\n",
    "*cars = list(arf.load('cars.arff'))   &nbsp;&nbsp;&nbsp;&nbsp;# Load your downloaded dataset (!curl, etc.)*\n",
    "\n",
    "*df = pd.DataFrame(cars)*  \n",
    "\n",
    "*There may be additional cleaning needed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training score was 1.0\n",
      "Average test score was 0.8170499999999998\n",
      "\n",
      "Scores for each chunk (as training, test):\n",
      "Chunk 0: 1.0, 0.7803\n",
      "Chunk 1: 1.0, 0.7919\n",
      "Chunk 2: 1.0, 0.8671\n",
      "Chunk 3: 1.0, 0.9711\n",
      "Chunk 4: 1.0, 0.8092\n",
      "Chunk 5: 1.0, 0.7746\n",
      "Chunk 6: 1.0, 0.8728\n",
      "Chunk 7: 1.0, 0.7977\n",
      "Chunk 8: 1.0, 0.8314\n",
      "Chunk 9: 1.0, 0.6744\n",
      "Output of best tree:\n",
      "safety = b'high', Information Gain = 0.238\n",
      "   persons = b'2', Information Gain = 0.473\n",
      "      prediction: b'unacc'\n",
      "   persons = b'4', Information Gain = 0.473\n",
      "      buying = b'high', Information Gain = 0.4\n",
      "         maint = b'high', Information Gain = 0.811\n",
      "            prediction: b'acc'\n",
      "         maint = b'low', Information Gain = 0.811\n",
      "            prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.811\n",
      "            prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.811\n",
      "            prediction: b'unacc'\n",
      "      buying = b'low', Information Gain = 0.4\n",
      "         maint = b'high', Information Gain = 0.778\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'vgood'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.778\n",
      "            lug_boot = b'big', Information Gain = 0.722\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 0.722\n",
      "               prediction: b'good'\n",
      "            lug_boot = b'small', Information Gain = 0.722\n",
      "               prediction: b'good'\n",
      "         maint = b'vhigh', Information Gain = 0.778\n",
      "            prediction: b'acc'\n",
      "      buying = b'med', Information Gain = 0.4\n",
      "         maint = b'high', Information Gain = 0.799\n",
      "            prediction: b'acc'\n",
      "         maint = b'low', Information Gain = 0.799\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'good'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'good'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'vgood'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'good'\n",
      "         maint = b'med', Information Gain = 0.799\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'vgood'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.799\n",
      "            prediction: b'acc'\n",
      "      buying = b'vhigh', Information Gain = 0.4\n",
      "         maint = b'high', Information Gain = 1.0\n",
      "            prediction: b'unacc'\n",
      "         maint = b'low', Information Gain = 1.0\n",
      "            prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 1.0\n",
      "            prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 1.0\n",
      "            prediction: b'unacc'\n",
      "   persons = b'more', Information Gain = 0.473\n",
      "      buying = b'high', Information Gain = 0.293\n",
      "         maint = b'high', Information Gain = 0.586\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "         maint = b'low', Information Gain = 0.586\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.586\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.586\n",
      "            prediction: b'unacc'\n",
      "      buying = b'low', Information Gain = 0.293\n",
      "         maint = b'high', Information Gain = 0.59\n",
      "            lug_boot = b'big', Information Gain = 0.74\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 0.74\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "            lug_boot = b'small', Information Gain = 0.74\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.59\n",
      "            lug_boot = b'big', Information Gain = 1.585\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 1.585\n",
      "               prediction: b'good'\n",
      "            lug_boot = b'small', Information Gain = 1.585\n",
      "               prediction: b'unacc'\n",
      "         maint = b'vhigh', Information Gain = 0.59\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "      buying = b'med', Information Gain = 0.293\n",
      "         maint = b'high', Information Gain = 0.748\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "         maint = b'low', Information Gain = 0.748\n",
      "            lug_boot = b'big', Information Gain = 0.74\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 0.74\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'good'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "            lug_boot = b'small', Information Gain = 0.74\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'good'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'good'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'good'\n",
      "         maint = b'med', Information Gain = 0.748\n",
      "            lug_boot = b'big', Information Gain = 0.74\n",
      "               prediction: b'vgood'\n",
      "            lug_boot = b'med', Information Gain = 0.74\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'vgood'\n",
      "            lug_boot = b'small', Information Gain = 0.74\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.748\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "      buying = b'vhigh', Information Gain = 0.293\n",
      "         maint = b'high', Information Gain = 0.788\n",
      "            prediction: b'unacc'\n",
      "         maint = b'low', Information Gain = 0.788\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.788\n",
      "            doors = b'2', Information Gain = 0.184\n",
      "               lug_boot = b'big', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               lug_boot = b'small', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.184\n",
      "               prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.788\n",
      "            prediction: b'unacc'\n",
      "safety = b'low', Information Gain = 0.238\n",
      "   prediction: b'unacc'\n",
      "safety = b'med', Information Gain = 0.238\n",
      "   persons = b'2', Information Gain = 0.269\n",
      "      prediction: b'unacc'\n",
      "   persons = b'4', Information Gain = 0.269\n",
      "      buying = b'high', Information Gain = 0.196\n",
      "         lug_boot = b'big', Information Gain = 0.366\n",
      "            maint = b'high', Information Gain = 0.811\n",
      "               prediction: b'acc'\n",
      "            maint = b'low', Information Gain = 0.811\n",
      "               prediction: b'acc'\n",
      "            maint = b'med', Information Gain = 0.811\n",
      "               prediction: b'acc'\n",
      "            maint = b'vhigh', Information Gain = 0.811\n",
      "               prediction: b'unacc'\n",
      "         lug_boot = b'med', Information Gain = 0.366\n",
      "            doors = b'2', Information Gain = 0.549\n",
      "               prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.549\n",
      "               prediction: b'unacc'\n",
      "            doors = b'4', Information Gain = 0.549\n",
      "               maint = b'high', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               maint = b'low', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               maint = b'med', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               maint = b'vhigh', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'5more', Information Gain = 0.549\n",
      "               maint = b'high', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               maint = b'low', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               maint = b'med', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               maint = b'vhigh', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "         lug_boot = b'small', Information Gain = 0.366\n",
      "            prediction: b'unacc'\n",
      "      buying = b'low', Information Gain = 0.196\n",
      "         maint = b'high', Information Gain = 0.469\n",
      "            prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.469\n",
      "            lug_boot = b'big', Information Gain = 0.918\n",
      "               prediction: b'good'\n",
      "            lug_boot = b'med', Information Gain = 0.918\n",
      "               prediction: b'acc'\n",
      "            lug_boot = b'small', Information Gain = 0.918\n",
      "               prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.469\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'unacc'\n",
      "      buying = b'med', Information Gain = 0.196\n",
      "         maint = b'high', Information Gain = 0.549\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'unacc'\n",
      "         maint = b'low', Information Gain = 0.549\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'good'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'good'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'good'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.549\n",
      "            prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.549\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'unacc'\n",
      "      buying = b'vhigh', Information Gain = 0.196\n",
      "         maint = b'high', Information Gain = 0.311\n",
      "            prediction: b'unacc'\n",
      "         maint = b'low', Information Gain = 0.311\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'unacc'\n",
      "         maint = b'med', Information Gain = 0.311\n",
      "            lug_boot = b'big', Information Gain = 0.667\n",
      "               prediction: b'acc'\n",
      "            lug_boot = b'med', Information Gain = 0.667\n",
      "               doors = b'2', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 1.0\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'4', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 1.0\n",
      "                  prediction: b'acc'\n",
      "            lug_boot = b'small', Information Gain = 0.667\n",
      "               prediction: b'unacc'\n",
      "         maint = b'vhigh', Information Gain = 0.311\n",
      "            prediction: b'unacc'\n",
      "   persons = b'more', Information Gain = 0.269\n",
      "      lug_boot = b'big', Information Gain = 0.236\n",
      "         maint = b'high', Information Gain = 0.349\n",
      "            buying = b'high', Information Gain = 0.811\n",
      "               prediction: b'acc'\n",
      "            buying = b'low', Information Gain = 0.811\n",
      "               prediction: b'acc'\n",
      "            buying = b'med', Information Gain = 0.811\n",
      "               prediction: b'acc'\n",
      "            buying = b'vhigh', Information Gain = 0.811\n",
      "               prediction: b'unacc'\n",
      "         maint = b'low', Information Gain = 0.349\n",
      "            buying = b'high', Information Gain = 0.918\n",
      "               prediction: b'acc'\n",
      "            buying = b'med', Information Gain = 0.918\n",
      "               prediction: b'good'\n",
      "            buying = b'vhigh', Information Gain = 0.918\n",
      "               prediction: b'acc'\n",
      "         maint = b'med', Information Gain = 0.349\n",
      "            buying = b'high', Information Gain = 0.391\n",
      "               prediction: b'acc'\n",
      "            buying = b'low', Information Gain = 0.391\n",
      "               prediction: b'good'\n",
      "            buying = b'med', Information Gain = 0.391\n",
      "               prediction: b'acc'\n",
      "            buying = b'vhigh', Information Gain = 0.391\n",
      "               prediction: b'acc'\n",
      "         maint = b'vhigh', Information Gain = 0.349\n",
      "            buying = b'high', Information Gain = 1.0\n",
      "               prediction: b'unacc'\n",
      "            buying = b'low', Information Gain = 1.0\n",
      "               prediction: b'acc'\n",
      "            buying = b'med', Information Gain = 1.0\n",
      "               prediction: b'acc'\n",
      "            buying = b'vhigh', Information Gain = 1.0\n",
      "               prediction: b'unacc'\n",
      "      lug_boot = b'med', Information Gain = 0.236\n",
      "         buying = b'high', Information Gain = 0.237\n",
      "            maint = b'high', Information Gain = 0.38\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'low', Information Gain = 0.38\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'med', Information Gain = 0.38\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'vhigh', Information Gain = 0.38\n",
      "               prediction: b'unacc'\n",
      "         buying = b'low', Information Gain = 0.237\n",
      "            doors = b'2', Information Gain = 0.197\n",
      "               maint = b'high', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               maint = b'med', Information Gain = 0.918\n",
      "                  prediction: b'acc'\n",
      "               maint = b'vhigh', Information Gain = 0.918\n",
      "                  prediction: b'unacc'\n",
      "            doors = b'3', Information Gain = 0.197\n",
      "               prediction: b'acc'\n",
      "            doors = b'4', Information Gain = 0.197\n",
      "               prediction: b'acc'\n",
      "            doors = b'5more', Information Gain = 0.197\n",
      "               prediction: b'acc'\n",
      "         buying = b'med', Information Gain = 0.237\n",
      "            maint = b'high', Information Gain = 0.591\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'low', Information Gain = 0.591\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'good'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'good'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'good'\n",
      "            maint = b'med', Information Gain = 0.591\n",
      "               prediction: b'acc'\n",
      "            maint = b'vhigh', Information Gain = 0.591\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "         buying = b'vhigh', Information Gain = 0.237\n",
      "            maint = b'high', Information Gain = 0.549\n",
      "               prediction: b'unacc'\n",
      "            maint = b'low', Information Gain = 0.549\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'med', Information Gain = 0.549\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'vhigh', Information Gain = 0.549\n",
      "               prediction: b'unacc'\n",
      "      lug_boot = b'small', Information Gain = 0.236\n",
      "         buying = b'high', Information Gain = 0.216\n",
      "            prediction: b'unacc'\n",
      "         buying = b'low', Information Gain = 0.216\n",
      "            maint = b'high', Information Gain = 0.558\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'med', Information Gain = 0.558\n",
      "               prediction: b'unacc'\n",
      "            maint = b'vhigh', Information Gain = 0.558\n",
      "               prediction: b'unacc'\n",
      "         buying = b'med', Information Gain = 0.216\n",
      "            maint = b'high', Information Gain = 0.549\n",
      "               prediction: b'unacc'\n",
      "            maint = b'low', Information Gain = 0.549\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'med', Information Gain = 0.549\n",
      "               doors = b'2', Information Gain = 0.811\n",
      "                  prediction: b'unacc'\n",
      "               doors = b'3', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'4', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "               doors = b'5more', Information Gain = 0.811\n",
      "                  prediction: b'acc'\n",
      "            maint = b'vhigh', Information Gain = 0.549\n",
      "               prediction: b'unacc'\n",
      "         buying = b'vhigh', Information Gain = 0.216\n",
      "            prediction: b'unacc'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 10-fold CV on Cars Dataset\n",
    "# Write a function that implements 10-fold cross validation\n",
    "cars_data, meta = arff.loadarff(\"data/cars.arff\")\n",
    "cars_data = np.array([[*row] for row in cars_data])\n",
    "cars_data\n",
    "chunks = np.array_split(cars_data, 10)\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "trees = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    data = np.concatenate((chunks[:i]+chunks[i+1:]), axis=0)\n",
    "    clfr = DTClassifier(data, meta.names())\n",
    "    clfr.fit()\n",
    "    training_scores.append(round(clfr.score(data), 4))\n",
    "    test_scores.append(round(clfr.score(chunk), 4))\n",
    "    trees.append(clfr.root)\n",
    "    \n",
    "\n",
    "# Report Training and Test Classification Accuracies\n",
    "print(f\"Average training score was {sum(training_scores)/len(training_scores)}\")\n",
    "print(f\"Average test score was {sum(test_scores)/len(test_scores)}\")\n",
    "print()\n",
    "print(\"Scores for each chunk (as training, test):\")\n",
    "for i, (train_score, test_score) in enumerate(zip(training_scores, test_scores)):\n",
    "    print(f\"Chunk {i}: {train_score}, {test_score}\")\n",
    "    \n",
    "print(\"Output of best tree:\")\n",
    "print(trees[test_scores.index(min(test_scores))])\n",
    "\n",
    "# print(scores)\n",
    "\n",
    "# Report Average Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Voting Dataset\n",
    "- Use this [Voting Dataset with missing values (voting_with_missing.arff)](https://byu.instructure.com/courses/14142/files?preview=4622298)\n",
    "- Note that you will need to support unknown attributes in the voting data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training score was 1.0\n",
      "Average test score was 0.94223\n",
      "\n",
      "Scores for each chunk (as training, test):\n",
      "Chunk 0: 1.0, 0.9773\n",
      "Chunk 1: 1.0, 0.9545\n",
      "Chunk 2: 1.0, 0.9773\n",
      "Chunk 3: 1.0, 0.9545\n",
      "Chunk 4: 1.0, 0.9773\n",
      "Chunk 5: 1.0, 0.9535\n",
      "Chunk 6: 1.0, 0.9302\n",
      "Chunk 7: 1.0, 0.8837\n",
      "Chunk 8: 1.0, 0.8605\n",
      "Chunk 9: 1.0, 0.9535\n",
      "Output of best tree:\n",
      "physician-fee-freeze = b'?', Information Gain = 0.786\n",
      "   mx-missile = b'?', Information Gain = 0.52\n",
      "      prediction: b'republican'\n",
      "   mx-missile = b'n', Information Gain = 0.52\n",
      "      prediction: b'democrat'\n",
      "   mx-missile = b'y', Information Gain = 0.52\n",
      "      anti-satellite-test-ban = b'?', Information Gain = 0.722\n",
      "         prediction: b'democrat'\n",
      "      anti-satellite-test-ban = b'n', Information Gain = 0.722\n",
      "         prediction: b'republican'\n",
      "      anti-satellite-test-ban = b'y', Information Gain = 0.722\n",
      "         prediction: b'democrat'\n",
      "physician-fee-freeze = b'n', Information Gain = 0.786\n",
      "   adoption-of-the-budget-resolution = b'?', Information Gain = 0.032\n",
      "      prediction: b'democrat'\n",
      "   adoption-of-the-budget-resolution = b'n', Information Gain = 0.032\n",
      "      education-spending = b'?', Information Gain = 0.227\n",
      "         prediction: b'republican'\n",
      "      education-spending = b'n', Information Gain = 0.227\n",
      "         duty-free-exports = b'?', Information Gain = 0.144\n",
      "            prediction: b'democrat'\n",
      "         duty-free-exports = b'n', Information Gain = 0.144\n",
      "            religious-groups-in-schools = b'?', Information Gain = 0.811\n",
      "               prediction: b'democrat'\n",
      "            religious-groups-in-schools = b'n', Information Gain = 0.811\n",
      "               prediction: b'republican'\n",
      "            religious-groups-in-schools = b'y', Information Gain = 0.811\n",
      "               prediction: b'democrat'\n",
      "         duty-free-exports = b'y', Information Gain = 0.144\n",
      "            prediction: b'democrat'\n",
      "      education-spending = b'y', Information Gain = 0.227\n",
      "         prediction: b'democrat'\n",
      "   adoption-of-the-budget-resolution = b'y', Information Gain = 0.032\n",
      "      prediction: b'democrat'\n",
      "physician-fee-freeze = b'y', Information Gain = 0.786\n",
      "   synfuels-corporation-cutback = b'?', Information Gain = 0.072\n",
      "      prediction: b'republican'\n",
      "   synfuels-corporation-cutback = b'n', Information Gain = 0.072\n",
      "      duty-free-exports = b'?', Information Gain = 0.057\n",
      "         prediction: b'republican'\n",
      "      duty-free-exports = b'n', Information Gain = 0.057\n",
      "         prediction: b'republican'\n",
      "      duty-free-exports = b'y', Information Gain = 0.057\n",
      "         immigration = b'n', Information Gain = 0.32\n",
      "            export-administration-act-south-africa = b'?', Information Gain = 0.5\n",
      "               water-project-cost-sharing = b'n', Information Gain = 1.0\n",
      "                  prediction: b'democrat'\n",
      "               water-project-cost-sharing = b'y', Information Gain = 1.0\n",
      "                  prediction: b'republican'\n",
      "            export-administration-act-south-africa = b'n', Information Gain = 0.5\n",
      "               prediction: b'republican'\n",
      "            export-administration-act-south-africa = b'y', Information Gain = 0.5\n",
      "               prediction: b'democrat'\n",
      "         immigration = b'y', Information Gain = 0.32\n",
      "            prediction: b'republican'\n",
      "   synfuels-corporation-cutback = b'y', Information Gain = 0.072\n",
      "      adoption-of-the-budget-resolution = b'?', Information Gain = 0.232\n",
      "         prediction: b'democrat'\n",
      "      adoption-of-the-budget-resolution = b'n', Information Gain = 0.232\n",
      "         education-spending = b'?', Information Gain = 0.203\n",
      "            prediction: b'republican'\n",
      "         education-spending = b'n', Information Gain = 0.203\n",
      "            prediction: b'democrat'\n",
      "         education-spending = b'y', Information Gain = 0.203\n",
      "            water-project-cost-sharing = b'?', Information Gain = 0.112\n",
      "               prediction: b'republican'\n",
      "            water-project-cost-sharing = b'n', Information Gain = 0.112\n",
      "               export-administration-act-south-africa = b'?', Information Gain = 0.322\n",
      "                  prediction: b'republican'\n",
      "               export-administration-act-south-africa = b'n', Information Gain = 0.322\n",
      "                  handicapped-infants = b'n', Information Gain = 1.0\n",
      "                     prediction: b'democrat'\n",
      "                  handicapped-infants = b'y', Information Gain = 1.0\n",
      "                     prediction: b'republican'\n",
      "               export-administration-act-south-africa = b'y', Information Gain = 0.322\n",
      "                  prediction: b'republican'\n",
      "            water-project-cost-sharing = b'y', Information Gain = 0.112\n",
      "               prediction: b'republican'\n",
      "      adoption-of-the-budget-resolution = b'y', Information Gain = 0.232\n",
      "         handicapped-infants = b'n', Information Gain = 0.971\n",
      "            prediction: b'democrat'\n",
      "         handicapped-infants = b'y', Information Gain = 0.971\n",
      "            prediction: b'republican'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Used 10-fold CV on Voting Dataset\n",
    "voting_data, meta = arff.loadarff(\"data/voting_with_missing.arff\")\n",
    "voting_data = np.array([[*row] for row in voting_data])\n",
    "voting_data\n",
    "chunks = np.array_split(voting_data, 10)\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "trees = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    data = np.concatenate((chunks[:i]+chunks[i+1:]), axis=0)\n",
    "    clfr = DTClassifier(data, meta.names())\n",
    "    clfr.fit()\n",
    "    training_scores.append(round(clfr.score(data), 4))\n",
    "    test_scores.append(round(clfr.score(chunk), 4))\n",
    "    trees.append(clfr.root)\n",
    "    \n",
    "\n",
    "# Report Training and Test Classification Accuracies\n",
    "print(f\"Average training score was {sum(training_scores)/len(training_scores)}\")\n",
    "print(f\"Average test score was {sum(test_scores)/len(test_scores)}\")\n",
    "print()\n",
    "print(\"Scores for each chunk (as training, test):\")\n",
    "for i, (train_score, test_score) in enumerate(zip(training_scores, test_scores)):\n",
    "    print(f\"Chunk {i}: {train_score}, {test_score}\")\n",
    "    \n",
    "print(\"Output of best tree:\")\n",
    "print(trees[test_scores.index(min(test_scores))])\n",
    "\n",
    "# print(scores)\n",
    "\n",
    "# Report Average Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Discuss Your Results\n",
    "\n",
    "- Summarize your results from both datasets, and discuss what you observed. \n",
    "- A fully expanded tree will often get 100% accuracy on the training set. Why does this happen and in what cases might it not?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        The results from each dataset had several unique characteristics from each other. As a result, part of this discussion appears in part <b>3.1</b> for each dataset. (sorry for the out-of-order; the discussion seemed to flow better that way.) To restate some, overall it seems that there were some attributes that played a big role in determining the outcome, while there were others that did a lot less. Both datsets ran into troubles with overfitting, where the path to the final decision on some branches passed through many specific questions. This was evidenced by the fact that that would often happen right next to a sister branch that more quickly came to a decision.<br>\n",
    "        &emsp; I discussed that one solution to this might be limiting the height of the decision tree. This could be done one of two ways: first the standard way which restricts the absolute height of the tree (/ depth of each node), or second not allowing the final decision in one branch to go too much deeper than one on a sister branch, which I'll call \"relative depth\". For instance, assuming a `maximum_relative_depth = 2`, on the cars dataset, the first split immediately finds a solution at `safety == low`. Thus, the sister branches (`safety == high` and `safety == med`) can only split `2` more times before being forced to give the most frequent answer. In this case this arbitrary number for `maximum_relative_depth` is perfect, since it allows the other branches to split on the more distinguishing features of `persons -> buying` (or in one case, `persons -> lug_boot`. \n",
    "    </li>\n",
    "    <br>\n",
    "    <li>\n",
    "        This happens just by virtue of fitting it to the training data, particularly if no pruning/limiting is used. It indicates the ever-present danger of overfitting, where a model will be highly tuned to always get the training data correctly, but not be generalizable to new data.\n",
    "    </li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (15%) For each of the two problems above, summarize in English what the decision tree has learned (i.e., look at the induced tree and describe what rules it has discovered to try to solve each task). \n",
    "- If the tree is very large you can just discuss a few of the more shallow attribute combinations and the most important decisions made high in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Discuss what the decision tree induced on the cars dataset has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On first glance, I noticed that the tree usually used most of the features to make a decision. To me, this is one of three reasons that hint at overfitting. The other two attributes are the large number of nodes (meaning almost all questions are usually asked), and the low accuracy which shows poor generalizability. Just looking at these features, it would seem that the features and attributes in this dataset are much less deterministic than those in the voting dataset, meaning that the decisions likely include an element of randomness (in other words, the \"gut feeling\" of the inspector).\n",
    "\n",
    "However, on closer inspection, the first three questions asked were always the same: `safety -> persons -> buying`. This indicates that these questions greatly impact the decisions made in the tree, whereas the others do not as much. This is further emphasized by the fact that the final outcomes after many of these third branches clearly had a most-frequent answer (given that it didn't converge on an answer earlier). So, some of the features clearly predicted more than the others.\n",
    "\n",
    "As a result, I think that this tree would greatly benefit from some sort of pruning, or limiting of tree depth. At the very least, taking the most common of each type after the first three questions would decrease the size of the tree, and would help with overfitting. However, this is only the case if there truly is an element to randomness. It is possible that I really don't understand cars, and so an experienced inspector really does distinguish to that fine level of detail. <em>(i.e. \"A car with `medium` safety, `more` than 4 people, a `medium` lug boot, `high` buying, and `high` maint is always acceptible unless it only has `2` doors.\") Nonetheless, this doesn't seem as likely due to the lower-than-expected accuracy with the testing data.\n",
    "\n",
    "## 3.2 Discuss what the decision tree induced on the voting dataset has learned\n",
    "\n",
    "Some of the attributes played a larger role in distingushing between the voter's ultimate decision. Some examples of these features include the physician fee freeze, adoption of budget resolutions, and education spending. I say this because these attributes were used in several different branch paths to distingush between the two groups. Other attributes, however, were only used sparingly to get a fine-tuned distinction between two groups. Some examples include religious groups in schools, handicapped infants, and water project cost sharing. My guess is this is because these attributes were fairly random, but during training after the data had been slimmed down significantly by other questions, it could be used to distingish between a handful of records. These results are likely not super meaningful, and would likely not be harmed too much by just taking the most common output at that point.\n",
    "\n",
    "Results that I found most interesting were ones that yielded opposite results based on a previous condition. For instance, given that an individual agreed with the physician wage freeze, if they agreed to increase education spending they were automatically classified as a democrat. However, given that the individual disagreed with physician wage freezes, if they agreed to increase education spending, the tree often calssified them as republican.\n",
    "\n",
    "Overall, this tree generalized better than the car dataset. To be honest, I'm not exactly sure why. If I had to guess, This might be because the attributes and features of this dataset were more deterministic than the cars dataset, which indicates that most questions would provide meaningful separation of the answer.  Though the voting dataset might benefit from decreasing the tree depth on the finer levels (see `physician-fee-freeze = y -> synfuels-corporation-cutback=n`, where only one path leads to a democrat--probably overfitted), getting a prediction often happend within 3-4 questions. Assuming that each answer occured in roughly equal proportion for each question (appropriate assumption?), this would indicate that the questions are often deterministic.\n",
    "\n",
    "## 3.3 How did you handle unknown attributes in the voting problem? Why did you choose this approach? (Do not use the approach of just throwing out data with unknown attributes).\n",
    "\n",
    "I chose to handle them by treating them as their own attribute. I did this because, in my mind, the fact that someone declined to give an answer to the question says something about their views. Each person may hold different views on different subjects, and so I thought it would be disingenuous to impute a value for them. \n",
    "\n",
    "Later on in my custom dataset, I felt that missing values in an `age` column would mess up the data more than an imputed value would. Because of that, I imputed the average value of the rest of the column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 (10%) Use Scikit Learn's decision tree on the voting dataset and compare your results. Try different parameters and report what parameters perform best on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 sklearn on Voting Dataset\n",
    "- Use this [Voting Dataset with missing values (voting_with_missing.arff)](https://byu.instructure.com/courses/14142/files?preview=4622298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9534883720930233"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn's Decision Tree to learn the voting dataset\n",
    "from sklearn import tree\n",
    "clfr = DecisionTreeClassifier()\n",
    "X, y = voting_data[:, :-1], voting_data[:, -1]\n",
    "X = np.array([[1 if val == b'y' else 0 if val == b'n' else -1 for val in row] for row in X])\n",
    "y = np.array([1 if val == b'republican' else 0 if val == b'democrat' else -1 for val in y])\n",
    "\n",
    "X_test, X = X[:len(X)//10,:], X[len(X)//10:, :]\n",
    "y_test, y = y[:len(y)//10], y[len(y)//10:]\n",
    "clfr = clfr.fit(X, y)\n",
    "clfr.score(X_test, y_test)\n",
    "# tree.plot_tree(clfr)\n",
    "# Explore different parameters\n",
    "\n",
    "# voting_data\n",
    "# Report results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss results & compare to your method's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 (10%) Choose a data set of your choice (not already used in this or previous labs) and use the sklearn decision tree to learn it. Experiment with different hyper-parameters to try to get the best results possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derm_clfr_0 score: 0.917\n",
      "derm_clfr_1 score: 0.944\n",
      "derm_clfr_2 score: 0.917\n",
      "derm_clfr_3 score: 1.0\n",
      "derm_clfr_4 score: 0.861\n",
      "derm_clfr_5 score: 0.944\n",
      "derm_clfr_6 score: 0.917\n",
      "best score =1.0\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn's Decision Tree on a new dataset\n",
    "derm_data = np.loadtxt(\"data/dermatology_data.csv\", delimiter=',', dtype=str)\n",
    "derm_metadata = np.loadtxt(\"data/dermatology_metadata.csv\", delimiter=',', dtype=str)\n",
    "## CLEAN DATA ##\n",
    "unknown_age_rows = np.where(derm_data[:,33]=='?')\n",
    "derm_data[unknown_age_rows,33] = '-1'# Remove unknown ages from col 33 ...\n",
    "derm_data = derm_data.astype(int)\n",
    "derm_data[unknown_age_rows, 33] = np.average(derm_data[np.invert(unknown_age_rows), 33])# ... and impute them with the average age (as an int)\n",
    "age_ranges = np.quantile(derm_data[:,33], [.25, .5, .75, 1])# Change age to separate into 4 quartile groups\n",
    "for i, quartile in enumerate(age_ranges):\n",
    "    derm_data[derm_data[:,33]<quartile,33] = i\n",
    "X, y = derm_data[:, :-1], derm_data[:, -1] # Split data into attributes and objectives...\n",
    "X_test, X = X[:len(X)//10,:], X[len(X)//10:, :] # ... then training and testing\n",
    "y_test, y = y[:len(y)//10], y[len(y)//10:]\n",
    "    \n",
    "#Fit and test\n",
    "derm_clfr_0 = DecisionTreeClassifier()\n",
    "derm_clfr_0.fit(X, y)\n",
    "\n",
    "derm_clfr_1 = DecisionTreeClassifier(splitter=\"random\")\n",
    "derm_clfr_1.fit(X, y)\n",
    "\n",
    "derm_clfr_2 = DecisionTreeClassifier(max_features=\"log2\")\n",
    "derm_clfr_2.fit(X, y)\n",
    "\n",
    "derm_clfr_3 = DecisionTreeClassifier(splitter=\"random\", max_features=\"log2\")\n",
    "derm_clfr_3.fit(X, y)\n",
    "\n",
    "derm_clfr_4 = DecisionTreeClassifier(max_depth=6)\n",
    "derm_clfr_4.fit(X, y)\n",
    "\n",
    "derm_clfr_5 = DecisionTreeClassifier(max_depth=6,splitter=\"random\")\n",
    "derm_clfr_5.fit(X, y)\n",
    "\n",
    "derm_clfr_6 = DecisionTreeClassifier(min_samples_split=5, splitter=\"random\")\n",
    "derm_clfr_6.fit(X, y)\n",
    "\n",
    "scores = [globals()[f\"derm_clfr_{i}\"].score(X_test, y_test) for i in range(7)]\n",
    "scores = [round(score,3) for score in scores]\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"derm_clfr_{i} score: {score}\")\n",
    "max_idx = scores.index(max(scores))\n",
    "print(f\"best score ={scores[max_idx]}\")\n",
    "best_tree = globals()[f\"derm_clfr_{max_idx}\"]\n",
    "\n",
    "# derm_data\n",
    "# Experiment with different hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (5%) Visualize sklearn's decision tree for your chosen data set (using export_graphviz or another tool) and discuss what you find. If your tree is too deep to reasonably fit on one page, show only the first few levels (e.g., top 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1232638/3472503716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Include decision tree visualization here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderm_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attribute {i}: {val}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# Include decision tree visualization here\n",
    "\n",
    "import graphviz\n",
    "for i, val in enumerate(derm_metadata):\n",
    "    print(f\"Attribute {i}: {val}\")\n",
    "print(\"=\"*20)\n",
    "result_key = [\"psoriasis\", \"seboreicdermatitis\", \"lichenplanus\", \"pityriasisrosea\", \"cronicdermatitis\", \"pityriasisrubrapilaris\"]\n",
    "for i, result in enumerate(result_key):\n",
    "    print(f\"Diagnosis {i}: {result}\")\n",
    "graphviz.Source(tree.export_graphviz(best_tree), format=\"png\")\n",
    "# Discuss what the model has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "I was quite impressed with the results! After training, the models would score between <em>85-98%</em> with some test data. I was surprised to see that the ones with the <b>most pruning</b> usually did better. I mean, I trusted y'all before, but it was kind of crazy to see overfitting happening in front of me.<br><br>\n",
    "\n",
    "The attributes chosed varied a bit (due to the random selection obviously), but the most common first split was on feature <b>19</b>, <em>\"Clubbing of the rete ridges\"</em>. Other common distinguishing factors were <b>26<b> (<em>\"Vasculation and damage of basal layer\"</em>), <b>27</b> (<em>\"Spongiosis\"</em>), and <b>30</b> (<em>perifollicular parakeratosis</em>). Unfortunately, it seems like the model was still a bit overfit, since many of the leaves only had one item in them at the end. This, I think, is because the dataset was quite wide; the number of attributes was only about a tenth of the number of examples. Thus, I think some degree of overfitting is inevitable without more data.<br><br>\n",
    "    \n",
    "I suppose now the biggest test is going to be running this by my uncle, a dermatologist. This Christmas break when he overheard I was going to be doing an AI class, he told me that the big idea in AI he's had for a while was, and I quote, \"like 20 questions but to diagnose skin cancers\". Perhaps with more data, it might be worth investing in little handheld devices to sell. :)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (optional 5% extra credit) Implement reduced error pruning to help avoid overfitting.  \n",
    "- You will need to take a validation set out of your training data to do this, while still having a test set to test your final accuracy. \n",
    "- Create a table comparing your decision tree implementation's results on the cars and voting data sets with and without reduced error pruning. \n",
    "- This table should compare:\n",
    "    - a) The # of nodes (including leaf nodes) and tree depth of the final decision trees \n",
    "    - b) The generalization (test set) accuracy. (For the unpruned 10-fold CV models, just use their average values in the table)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
